= Problems and Questions
While reinforcement learning has been studied in many technical applications, there have been fewer applications for human-centered contexts where decision have real consequences. Compared to supervised learnings models that have been systematically analyzed in regard to bias and fairness, reinforcement learning introduces some unique challenges. Sequential decision-making and feedback-loops can influence the output and performance of model and are critical aspects to consider for bias and fairness measurement. How model complexity affects bias is unclear. Complex models such as Deep Q-Networks (DQN), that learn trough multi-step decision policies, can behave differently than one-step models such as Contextual Bandits. A comparative analysis of reinforcement models in human-centered datasets is motivated by the gap of knowledge, with the goal to better understand where fairness issues stem from.

The key question in this research is how DQN and Conceptual Bandits compare in terms of bias and fairness, when trained on the same dataset that stems from the healthcare field. To break down this question, we examine several related aspects. Does the model complexity contribute to bias? Are sensitive features such as age, gender, ethnicity more vulnerable than others? How does dataset attributes like size, and distribution of groups have effect on results. Does the sequential decision-making of DQN lead to accumulation of bias over time compared to contextual bandits, that lack time dependencies? And finally, how does the design of reward systems effect the fairness?

The aim of the study is to attain better understanding of how chosen reinforcement learning models effect bias and fairness in human-centered applications. However, goal is not only to identify a problem, but to provide useful guidelines for when choosing and developing a reinforcement learning model that is responsible for decision-making in real world applications and aims to be fair. 
