
@book{sutton_reinforcement_1998,
	title = {Reinforcement learning: {An} introduction},
	volume = {1},
	publisher = {MIT press Cambridge},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {1998},
	note = {Issue: 1},
}

@article{neufeld_enforcing_2022,
	title = {Enforcing ethical goals over reinforcement-learning policies},
	volume = {24},
	number = {4},
	journal = {Ethics and Information Technology},
	author = {Neufeld, Emery A. and Bartocci, Ezio and Ciabattoni, Agata and Governatori, Guido},
	year = {2022},
	note = {ISBN: 1388-1957
Publisher: Springer},
	pages = {43},
}

@article{yang_algorithmic_2023,
	title = {Algorithmic fairness and bias mitigation for clinical machine learning with deep reinforcement learning},
	volume = {5},
	number = {8},
	journal = {Nature Machine Intelligence},
	author = {Yang, Jenny and Soltan, Andrew AS and Eyre, David W. and Clifton, David A.},
	year = {2023},
	note = {ISBN: 2522-5839
Publisher: Nature Publishing Group UK London},
	pages = {884--894},
}

@article{jayaraman_primer_2024,
	title = {A primer on reinforcement learning in medicine for clinicians},
	volume = {7},
	number = {1},
	journal = {NPJ Digital Medicine},
	author = {Jayaraman, Pushkala and Desman, Jacob and Sabounchi, Moein and Nadkarni, Girish N. and Sakhuja, Ankit},
	year = {2024},
	note = {ISBN: 2398-6352
Publisher: Nature Publishing Group UK London},
	pages = {337},
}

@article{mehrabi_survey_2021,
	title = {A survey on bias and fairness in machine learning},
	volume = {54},
	number = {6},
	journal = {ACM computing surveys (CSUR)},
	author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
	year = {2021},
	note = {ISBN: 0360-0300
Publisher: ACM New York, NY, USA},
	pages = {1--35},
}

@incollection{abhivardhan_data_2025,
	title = {Data {Governance}},
	booktitle = {Handbook of {Human}-{Centered} {Artificial} {Intelligence}},
	publisher = {Springer},
	author = {Abhivardhan},
	year = {2025},
	pages = {1--61},
}

@article{cannelli_hedging_2023,
	title = {Hedging using reinforcement learning: {Contextual} k-armed bandit versus {Q}-learning},
	volume = {9},
	journal = {The Journal of Finance and Data Science},
	author = {Cannelli, Loris and Nuti, Giuseppe and Sala, Marzio and Szehr, Oleg},
	year = {2023},
	note = {ISBN: 2405-9188
Publisher: Elsevier},
	pages = {100101},
}

@article{huang_scoping_2024,
	title = {A scoping review of fair machine learning techniques when using real-world data},
	volume = {151},
	journal = {Journal of biomedical informatics},
	author = {Huang, Yu and Guo, Jingchuan and Chen, Wei-Han and Lin, Hsin-Yueh and Tang, Huilin and Wang, Fei and Xu, Hua and Bian, Jiang},
	year = {2024},
	note = {ISBN: 1532-0464
Publisher: Elsevier},
	pages = {104622},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	number = {7540},
	journal = {nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg},
	year = {2015},
	note = {ISBN: 1476-4687
Publisher: Nature Publishing Group},
	pages = {529--533},
}

@article{chawla_smote_2002,
	title = {{SMOTE}: synthetic minority over-sampling technique},
	volume = {16},
	journal = {Journal of artificial intelligence research},
	author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, W. Philip},
	year = {2002},
	note = {ISBN: 1076-9757},
	pages = {321--357},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: {Machine} learning in {Python}},
	volume = {12},
	journal = {the Journal of machine Learning research},
	author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent},
	year = {2011},
	note = {ISBN: 1532-4435
Publisher: JMLR. org},
	pages = {2825--2830},
}

@article{saito_precision-recall_2015,
	title = {The precision-recall plot is more informative than the {ROC} plot when evaluating binary classifiers on imbalanced datasets},
	volume = {10},
	number = {3},
	journal = {PloS one},
	author = {Saito, Takaya and Rehmsmeier, Marc},
	year = {2015},
	note = {ISBN: 1932-6203
Publisher: Public Library of Science San Francisco, CA USA},
	pages = {e0118432},
}

@article{jui_fairness_2024,
	title = {Fairness issues, current approaches, and challenges in machine learning models},
	volume = {15},
	number = {8},
	journal = {International Journal of Machine Learning and Cybernetics},
	author = {Jui, Tonni Das and Rivas, Pablo},
	year = {2024},
	note = {ISBN: 1868-8071
Publisher: Springer},
	pages = {3095--3125},
}

@article{abdellatif_reinforcement_2023,
	title = {Reinforcement learning for intelligent healthcare systems: {A} review of challenges, applications, and open research issues},
	volume = {10},
	number = {24},
	journal = {IEEE Internet of Things Journal},
	author = {Abdellatif, Alaa Awad and Mhaisen, Naram and Mohamed, Amr and Erbad, Aiman and Guizani, Mohsen},
	year = {2023},
	note = {ISBN: 2327-4662
Publisher: IEEE},
	pages = {21982--22007},
}


@article{rahman_data_2024,
	title = {Data {Quality}, {Bias}, and {Strategic} {Challenges} in {Reinforcement} {Learning} for {Healthcare}: {A} {Survey}},
	volume = {3},
	copyright = {Copyright (c) 2024 Atta Ur Rahman, Bibi Saqia, Yousef S. Alsenani, Inam Ullah},
	issn = {2583-6250},
	shorttitle = {Data {Quality}, {Bias}, and {Strategic} {Challenges} in {Reinforcement} {Learning} for {Healthcare}},
	url = {https://ijdiic.com/index.php/research/article/view/128},
	doi = {10.59461/ijdiic.v3i3.128},
	abstract = {Data quality is a critical aspect of data analytics since it directly influences the accuracy and effectiveness of insights and predictions generated from data. Artificial Intelligence (AI) schemes have grown in the existing era of technological advancement, which provides innovative exposure to healthcare applications. Reinforcement Learning (RL) is a subfield and an influential Machine Learning (ML) model aimed at optimizing decision-making by association with dynamic environments. In healthcare applications, RL can modify conduct strategies, enhance source application, and improve patient investigation history by using various data modalities. The worth of the data quality regulates how effective RL is in healthcare applications. In healthcare, the model predictions have a direct impact on patient's lives, and poor data quality often leads to wrong evaluations that expose patient safety and treatment quality. Biases in data quality have also presented a challenging influence on the RL model's effectiveness and accuracy. RL models have enormous potential in healthcare; however, various strategic limitations prevent their widespread acceptance and deployment. The implementation of RL in healthcare faces serious issues, mostly around data quality, bias, and tactical difficulties. This study delivers a broad survey of these challenges, emphasizing how imbalanced, imperfect, and biased data can affect the generalizability and performance of RL models. We critically assessed the sources of data bias, comprising demographic imbalances and irregularities in electronic health records (EHRs), and their impact on RL algorithms. This survey aims to present a detailed study of the complex circumstances relating to data quality, data biases, and strategic barriers in RL models deploying in healthcare applications. However, the main contribution of the proposed study is that it provides a systematic review of these challenges and delivers a roadmap for future work intended to refine the consistency, fairness, and scalability of RL in healthcare sectors.},
	language = {en},
	number = {3},
	urldate = {2025-09-14},
	journal = {International Journal of Data Informatics and Intelligent Computing},
	author = {Rahman, Atta Ur and Saqia, Bibi and Alsenani, Yousef S. and Ullah, Inam},
	month = sep,
	year = {2024},
	keywords = {Bias Issues, Data Quality, Healthcare Applications, Reinforcement Learning, Strategic Obstacles},
	pages = {24--42},
	file = {PDF:/home/aniqu/Zotero/storage/72STQKEJ/Rahman et al. - 2024 - Data Quality, Bias, and Strategic Challenges in Reinforcement Learning for Healthcare A Survey.pdf:application/pdf},
}

@article{smith_bias_2023,
	title = {Bias in {Reinforcement} {Learning}: {A} {Review} in {Healthcare} {Applications}},
	volume = {56},
	issn = {0360-0300},
	shorttitle = {Bias in {Reinforcement} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/3609502},
	doi = {10.1145/3609502},
	abstract = {Reinforcement learning (RL) can assist in medical decision making using patient data collected in electronic health record (EHR) systems. RL, a type of machine learning, can use these data to develop treatment policies. However, RL models are typically trained using imperfect retrospective EHR data. Therefore, if care is not taken in training, RL policies can propagate existing bias in healthcare. Literature that considers and addresses the issues of bias and fairness in sequential decision making are reviewed. The major themes to mitigate bias that emerge relate to (1) data management; (2) algorithmic design; and (3) clinical understanding of the resulting policies.},
	number = {2},
	urldate = {2025-09-19},
	journal = {ACM Comput. Surv.},
	author = {Smith, Benjamin and Khojandi, Anahita and Vasudevan, Rama},
	month = sep,
	year = {2023},
	pages = {52:1--52:17},
	file = {Full Text PDF:/home/aniqu/Zotero/storage/8XI2GANS/Smith et al. - 2023 - Bias in Reinforcement Learning A Review in Healthcare Applications.pdf:application/pdf},
}

@article{badidi_edge_2023,
	title = {Edge {AI} for {Early} {Detection} of {Chronic} {Diseases} and the {Spread} of {Infectious} {Diseases}: {Opportunities}, {Challenges}, and {Future} {Directions}},
	volume = {15},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1999-5903},
	shorttitle = {Edge {AI} for {Early} {Detection} of {Chronic} {Diseases} and the {Spread} of {Infectious} {Diseases}},
	url = {https://www.mdpi.com/1999-5903/15/11/370},
	doi = {10.3390/fi15110370},
	abstract = {Edge AI, an interdisciplinary technology that enables distributed intelligence with edge devices, is quickly becoming a critical component in early health prediction. Edge AI encompasses data analytics and artificial intelligence (AI) using machine learning, deep learning, and federated learning models deployed and executed at the edge of the network, far from centralized data centers. AI enables the careful analysis of large datasets derived from multiple sources, including electronic health records, wearable devices, and demographic information, making it possible to identify intricate patterns and predict a person’s future health. Federated learning, a novel approach in AI, further enhances this prediction by enabling collaborative training of AI models on distributed edge devices while maintaining privacy. Using edge computing, data can be processed and analyzed locally, reducing latency and enabling instant decision making. This article reviews the role of Edge AI in early health prediction and highlights its potential to improve public health. Topics covered include the use of AI algorithms for early detection of chronic diseases such as diabetes and cancer and the use of edge computing in wearable devices to detect the spread of infectious diseases. In addition to discussing the challenges and limitations of Edge AI in early health prediction, this article emphasizes future research directions to address these concerns and the integration with existing healthcare systems and explore the full potential of these technologies in improving public health.},
	language = {en},
	number = {11},
	urldate = {2025-09-19},
	journal = {Future Internet},
	author = {Badidi, Elarbi},
	month = nov,
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {artificial intelligence, chronic diseases, data analysis, data privacy, early health prediction, edge computing, federated learning, healthcare informatics, public health, wearable devices},
	pages = {370},
	file = {Full Text PDF:/home/aniqu/Zotero/storage/X2GKULHV/Badidi - 2023 - Edge AI for Early Detection of Chronic Diseases and the Spread of Infectious Diseases Opportunities.pdf:application/pdf},
}

@article{oh_reinforcement_2022,
	title = {Reinforcement learning-based expanded personalized diabetes treatment recommendation using {South} {Korean} electronic health records},
	volume = {206},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422011733},
	doi = {10.1016/j.eswa.2022.117932},
	abstract = {Currently, electronic medical records are becoming more accessible to a growing number of researchers seeking to develop personalized healthcare recommendations to aid physicians in making better clinical decisions and treating patients. As a result, clinical decision research has become more focused on data-driven optimization. In this study, we analyze Korean patients' electronic health records—including medical history, medications, laboratory tests, and more information—shared by the national health insurance system. We aim to develop a reinforcement learning-based expanded treatment recommendation model using the health records of South Korean citizens to assist physicians. This study is significant in that expert and intelligent systems harmoniously solve the problem that directly addresses many clinical challenges in prescribing proper diabetes medication when assessing the physical state of diabetes patients. Reinforcement learning is a mechanism for determining how agents should behave in a given environment to maximize a cumulative reward. The basic model for a reinforcement learning design environment is the Markov decision process (MDP) model. Although it is effective and easy to use, the MDP model is limited by dimensionality, i.e., many details about the patients cannot be considered when building the model. To address this issue, we applied a contextual bandits approach to create a more practical model that can expand states and actions by considering several details that are crucial for patients with diabetes. Finally, we validated the performance of the proposed contextual bandits model by comparing it with existing reinforcement-learning algorithms.},
	urldate = {2025-09-17},
	journal = {Expert Systems with Applications},
	author = {Oh, Sang Ho and Park, Jongyoul and Lee, Su Jin and Kang, Seungyeon and Mo, Jeonghoon},
	month = nov,
	year = {2022},
	keywords = {Data-driven optimization, Decision making, Electronic health records, Precision medicine, Reinforcement learning},
	pages = {117932},
	file = {ScienceDirect Full Text PDF:/home/aniqu/Zotero/storage/8HWDWEW3/Oh et al. - 2022 - Reinforcement learning-based expanded personalized diabetes treatment recommendation using South Kor.pdf:application/pdf;ScienceDirect Snapshot:/home/aniqu/Zotero/storage/UPUPGG65/S0957417422011733.html:text/html},
}

@article{dimakopoulou_balanced_2019,
	title = {Balanced {Linear} {Contextual} {Bandits}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4221},
	doi = {10.1609/aaai.v33i01.33013445},
	abstract = {Contextual bandit algorithms are sensitive to the estimation method of the outcome model as well as the exploration method used, particularly in the presence of rich heterogeneity or complex outcome models, which can lead to difficult estimation problems along the path of learning. We develop algorithms for contextual bandits with linear payoffs that integrate balancing methods from the causal inference literature in their estimation to make it less prone to problems of estimation bias. We provide the first regret bound analyses for linear contextual bandits with balancing and show that our algorithms match the state of the art theoretical guarantees. We demonstrate the strong practical advantage of balanced contextual bandits on a large number of supervised learning datasets and on a synthetic example that simulates model misspecification and prejudice in the initial training data.},
	language = {en},
	number = {01},
	urldate = {2025-09-17},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Dimakopoulou, Maria and Zhou, Zhengyuan and Athey, Susan and Imbens, Guido},
	month = jul,
	year = {2019},
	pages = {3445--3453},
	file = {Full Text PDF:/home/aniqu/Zotero/storage/UVVJPEFL/Dimakopoulou et al. - 2019 - Balanced Linear Contextual Bandits.pdf:application/pdf},
}

@article{varatharajah_contextual-bandit-based_2022,
	title = {A {Contextual}-{Bandit}-{Based} {Approach} for {Informed} {Decision}-{Making} in {Clinical} {Trials}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2075-1729},
	url = {https://www.mdpi.com/2075-1729/12/8/1277},
	doi = {10.3390/life12081277},
	abstract = {Clinical trials are conducted to evaluate the efficacy of new treatments. Clinical trials involving multiple treatments utilize the randomization of treatment assignments to enable the evaluation of treatment efficacies in an unbiased manner. Such evaluation is performed in post hoc studies that usually use supervised-learning methods that rely on large amounts of data collected in a randomized fashion. That approach often proves to be suboptimal in that some participants may suffer and even die as a result of having not received the most appropriate treatments during the trial. Reinforcement-learning methods improve the situation by making it possible to learn the treatment efficacies dynamically during the course of the trial, and to adapt treatment assignments accordingly. Recent efforts using multi-arm bandits, a type of reinforcement-learning method, have focused on maximizing clinical outcomes for a population that was assumed to be homogeneous. However, those approaches have failed to account for the variability among participants that is becoming increasingly evident as a result of recent clinical-trial-based studies. We present a contextual-bandit-based online treatment optimization algorithm that, in choosing treatments for new participants in the study, takes into account not only the maximization of the clinical outcomes as well as the patient characteristics. We evaluated our algorithm using a real clinical trial dataset from the International Stroke Trial. We simulated the online setting by sequentially going through the data of each participant admitted to the trial. Two bandits (one for each context) were created, with four choices of treatments. For a new participant in the trial, depending on the context, one of the bandits was selected. Then, we took three different approaches to choose a treatment: (a) a random choice (i.e., the strategy currently used in clinical trial settings), (b) a Thompson sampling-based approach, and (c) a UCB-based approach. Success probabilities of each context were calculated separately by considering the participants with the same context. Those estimated outcomes were used to update the prior distributions within the bandit corresponding to the context of each participant. We repeated that process through the end of the trial and recorded the outcomes and the chosen treatments for each approach. We also evaluated a context-free multi-arm-bandit-based approach, using the same dataset, to showcase the benefits of our approach. In the context-free case, we calculated the success probabilities for the Bernoulli sampler using the whole clinical trial dataset in a context-independent manner. The results of our retrospective analysis indicate that the proposed approach performs significantly better than either a random assignment of treatments (the current gold standard) or a multi-arm-bandit-based approach, providing substantial gains in the percentage of participants who are assigned the most suitable treatments. The contextual-bandit and multi-arm bandit approaches provide 72.63\% and 64.34\% gains, respectively, compared to a random assignment.},
	language = {en},
	number = {8},
	urldate = {2025-09-17},
	journal = {Life},
	author = {Varatharajah, Yogatheesan and Berry, Brent},
	month = aug,
	year = {2022},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {bandits, clinical trials, machine learning},
	pages = {1277},
	file = {Full Text PDF:/home/aniqu/Zotero/storage/GSTBWHSR/Varatharajah and Berry - 2022 - A Contextual-Bandit-Based Approach for Informed Decision-Making in Clinical Trials.pdf:application/pdf},
}

@article{keyvanshokooh_contextual_2025,
	title = {Contextual {Learning} with {Online} {Convex} {Optimization}: {Theory} and {Application} to {Medical} {Decision}-{Making}},
	shorttitle = {Contextual {Learning} with {Online} {Convex} {Optimization}},
	doi = {10.1287/mnsc.2019.03211},
	abstract = {Optimizing the treatment regimen is a fundamental medical decision-making problem. This can be thought of as a two-dimensional decision-making problem with a nested structure, because it involves determining both the optimal medication and its optimal dose. Identifying the most effective medication for an individual often poses considerable difficulty, and even when a suitable medication is ascertained, dosing it optimally
remains a significant challenge. Making these two nested decisions necessitates the adaptive learning of a personalized disease progression control model. To address this problem, we propose a novel contextual multi-armed bandit model under a two-dimensional control with a nested structure. For this model, we develop a
new joint contextual learning and optimization algorithm, termed the stochastic sub-gradient descent atop contextual multi-armed bandit (SGD-MAB) algorithm. It sequentially selects for a patient: (i) the best medication based on their contextual information, and (ii) the corresponding dose optimized over the prior history of those patients who received the same medication. We prove that it admits a sub-linear regret, which is tight up to a logarithmic factor. Our regret analysis leverages the strengths of both contextual
bandit approaches and online convex optimization techniques in a seamless fashion. We substantiate the practicality of SGD-MAB using clinical data on patients with hypertension and heightened cardiovascular risks. Our analysis indicates that SGD-MAB has the potential to surpass current practices. We benchmark several policies to show the advantages of our approach and offer critical insights. Our framework holds promise for various applications beyond healthcare that require nested decision-making.},
	journal = {Management Science},
	author = {Keyvanshokooh, Esmaeil and Zhalechian, Mohammad and Shi, Cong and Van Oyen, Mark and Kazemian, Pooyan},
	month = apr,
	year = {2025},
	file = {Full Text PDF:/home/aniqu/Zotero/storage/XHKEB5UW/Keyvanshokooh et al. - 2025 - Contextual Learning with Online Convex Optimization Theory and Application to Medical Decision-Maki.pdf:application/pdf},
}

@inproceedings{xu_contextual_2025,
	address = {Singapore},
	title = {Contextual {Bandit} with {Herding} {Effects}: {Algorithms} and {Recommendation} {Applications}},
	isbn = {978-981-96-0128-8},
	shorttitle = {Contextual {Bandit} with {Herding} {Effects}},
	doi = {10.1007/978-981-96-0128-8_12},
	abstract = {Contextual bandits serve as a fundamental algorithmic framework for optimizing recommendation decisions online. Though extensive attention has been paid to tailoring contextual bandits for recommendation applications, the “herding effects” in user feedback have been ignored. These herding effects bias user feedback toward historical ratings, breaking down the assumption of unbiased feedback inherent in contextual bandits. This paper develops a novel variant of the contextual bandit that is tailored to address the feedback bias caused by the herding effects. A user feedback model is formulated to capture this feedback bias. We design the TS-Conf (Thompson Sampling under Conformity) algorithm, which employs posterior sampling to balance the exploration and exploitation tradeoff. We prove an upper bound for the regret of the algorithm, revealing the impact of herding effects on learning speed. Extensive experiments on datasets demonstrate that TS-Conf outperforms four benchmark algorithms. Analysis reveals that TS-Conf effectively mitigates the negative impact of herding effects, resulting in faster learning and improved recommendation accuracy.},
	language = {en},
	booktitle = {{PRICAI} 2024: {Trends} in {Artificial} {Intelligence}},
	publisher = {Springer Nature},
	author = {Xu, Luyue and Wang, Liming and Xie, Hong and Zhou, Mingqiang},
	editor = {Hadfi, Rafik and Anthony, Patricia and Sharma, Alok and Ito, Takayuki and Bai, Quan},
	year = {2025},
	keywords = {contextual bandits, herding effects, recommendation},
	pages = {132--144},
}

