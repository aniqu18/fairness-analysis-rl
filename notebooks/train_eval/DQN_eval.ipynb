{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eca21663",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfa9007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# === Paths ===\n",
    "MODELS_DIR = \"../../models\"\n",
    "MODEL_ORIG_PKL = f\"{MODELS_DIR}/dqn_original.pkl\"\n",
    "MODEL_RES_PKL = f\"{MODELS_DIR}/dqn_resampled.pkl\"\n",
    "\n",
    "\n",
    "# === Load models ===\n",
    "# Load from pickle\n",
    "with open(MODEL_ORIG_PKL, \"rb\") as f:\n",
    "    model_orig = pickle.load(f)\n",
    "with open(MODEL_RES_PKL, \"rb\") as f:\n",
    "    model_res = pickle.load(f)\n",
    "\n",
    "# === Evaluation helper ===\n",
    "def evaluate_model(model, X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        preds = model(X_tensor).argmax(dim=1).cpu().numpy()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7714242d",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9435ea1",
   "metadata": {},
   "source": [
    "### General accuracy resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6629eb27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11956521739130435"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.read_csv(f\"../../data/X_test.csv\").values.astype(np.float32)\n",
    "y_test = pd.read_csv(f\"../../data/y_test.csv\")\n",
    "\n",
    "preds = evaluate_model(model_res, X_test)\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2d18b",
   "metadata": {},
   "source": [
    "### Gender resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "738188e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            accuracy  recall        f1  n_samples\n",
      "Female (0)  0.138158    0.20  0.048555      152.0\n",
      "Male (1)    0.031250    0.25  0.015625       32.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniqu/.conda/envs/fairness-analysis-rl/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "\n",
    "X_test = pd.read_csv('../../data/X_test.csv')\n",
    "y_test = pd.read_csv('../../data/y_test.csv')\n",
    "\n",
    "X_test_num = X_test.values.astype(np.float32)\n",
    "\n",
    "preds_test = evaluate_model(model_res, X_test_num)\n",
    "\n",
    "X_test[\"pred\"] = preds_test       \n",
    "X_test[\"true\"] = y_test   # ground truth\n",
    "\n",
    "# Protected attribute\n",
    "protected_attr = \"sex_Male\"  # since it's 0/1 after encoding\n",
    "groups = X_test[protected_attr].unique()\n",
    "\n",
    "metrics = {}\n",
    "for g in groups:\n",
    "    group_df = X_test[X_test[protected_attr] == g]\n",
    "    n_samples = len(group_df)\n",
    "    acc = accuracy_score(group_df[\"true\"], group_df[\"pred\"])\n",
    "    rec = recall_score(group_df[\"true\"], group_df[\"pred\"], average=\"macro\")  # across all 5 classes\n",
    "    f1 = f1_score(group_df[\"true\"], group_df[\"pred\"], average=\"macro\")  \n",
    "    metrics[g] = {\"accuracy\": acc, \"recall\": rec, \"f1\": f1, \"n_samples\": n_samples}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "metrics_df.index = [\"Female (0)\", \"Male (1)\"] # type: ignore\n",
    "print(metrics_df)\n",
    "\n",
    "# save\n",
    "import json\n",
    "\n",
    "with open(\"../../results/dqn_gender_resampled.json\", \"w\") as f:\n",
    "    json.dump(metrics_df.to_dict(), f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairness-analysis-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
