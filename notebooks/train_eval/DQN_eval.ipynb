{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eca21663",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfa9007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# === Paths ===\n",
    "MODELS_DIR = \"../../models\"\n",
    "MODEL_ORIG_PKL = f\"{MODELS_DIR}/dqn_original.pkl\"\n",
    "MODEL_RES_PKL = f\"{MODELS_DIR}/dqn_resampled.pkl\"\n",
    "\n",
    "\n",
    "# === Load models ===\n",
    "# Load from pickle\n",
    "with open(MODEL_ORIG_PKL, \"rb\") as f:\n",
    "    model_orig = pickle.load(f)\n",
    "with open(MODEL_RES_PKL, \"rb\") as f:\n",
    "    model_res = pickle.load(f)\n",
    "\n",
    "# === Evaluation helper ===\n",
    "def evaluate_model(model, X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        preds = model(X_tensor).argmax(dim=1).cpu().numpy()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7714242d",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9435ea1",
   "metadata": {},
   "source": [
    "### General accuracy resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6629eb27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11956521739130435"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.read_csv(f\"../../data/X_test.csv\").values.astype(np.float32)\n",
    "y_test = pd.read_csv(f\"../../data/y_test.csv\")\n",
    "\n",
    "preds = evaluate_model(model_res, X_test)\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2d18b",
   "metadata": {},
   "source": [
    "### Gender resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738188e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        accuracy  recall      f1  n_samples\n",
      "Male      0.1382    0.20  0.0486      152.0\n",
      "Female    0.0312    0.25  0.0156       32.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniqu/.conda/envs/fairness-analysis-rl/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "\n",
    "X_test = pd.read_csv('../../data/X_test.csv')\n",
    "y_test = pd.read_csv('../../data/y_test.csv')\n",
    "\n",
    "X_test_num = X_test.values.astype(np.float32)\n",
    "\n",
    "preds_test = evaluate_model(model_res, X_test_num)\n",
    "\n",
    "X_test[\"pred\"] = preds_test       \n",
    "X_test[\"true\"] = y_test   \n",
    "\n",
    "# Protected attribute\n",
    "protected_attr = \"sex_Male\"  \n",
    "groups = X_test[protected_attr].unique()\n",
    "\n",
    "metrics = {}\n",
    "for g in groups:\n",
    "    group_df = X_test[X_test[protected_attr] == g]\n",
    "    n_samples = len(group_df)\n",
    "    acc = accuracy_score(group_df[\"true\"], group_df[\"pred\"])\n",
    "    rec = recall_score(group_df[\"true\"], group_df[\"pred\"], average=\"macro\")  \n",
    "    f1 = f1_score(group_df[\"true\"], group_df[\"pred\"], average=\"macro\")  \n",
    "    label = \"Male\" if g == 1 else \"Female\"\n",
    "    metrics[label] = {\n",
    "        \"accuracy\": round(acc, 4),\n",
    "        \"recall\": round(rec, 4),\n",
    "        \"f1\": round(f1, 4),\n",
    "        \"n_samples\": int(n_samples)\n",
    "    }\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "print(metrics_df)\n",
    "\n",
    "# save\n",
    "import json\n",
    "\n",
    "with open(\"../../results/dqn_gender_resampled.json\", \"w\") as f:\n",
    "    json.dump(metrics_df.to_dict(), f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6acd7c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = 1.694, p = 0.09024\n",
      "accuracy difference NOT significant\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "\n",
    "male_mask = X_test[\"sex_Male\"] == 1\n",
    "female_mask = X_test[\"sex_Male\"] == 0\n",
    "\n",
    "\n",
    "y_true_male = y_test[male_mask]\n",
    "y_pred_male = preds_test[male_mask]\n",
    "y_true_female = y_test[female_mask]\n",
    "y_pred_female = preds_test[female_mask]\n",
    "\n",
    "assert((len(y_true_male) + len(y_true_female)) == (len(y_pred_male) + len(y_pred_female)))\n",
    "\n",
    "acc_male = (y_pred_male == y_true_male.to_numpy().ravel()).astype(int)\n",
    "acc_female = (y_pred_female == y_true_female.to_numpy().ravel()).astype(int)\n",
    "\n",
    "\n",
    "count = np.array([acc_male.sum(), acc_female.sum()])  # number of correct predictions\n",
    "nobs = np.array([len(acc_male), len(acc_female)])      # group sizes\n",
    "\n",
    "stat, pval = proportions_ztest(count, nobs)\n",
    "print(f\"z = {stat:.3f}, p = {pval:.5f}\")\n",
    "\n",
    "if pval < 0.05:\n",
    "    print('accuracy difference significant')\n",
    "else:\n",
    "    print('accuracy difference NOT significant')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6e485d",
   "metadata": {},
   "source": [
    "### General accuracy original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "150cc845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44565217391304346"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.read_csv(f\"../../data/X_test.csv\").values.astype(np.float32)\n",
    "y_test = pd.read_csv(f\"../../data/y_test.csv\")\n",
    "\n",
    "preds = evaluate_model(model_orig, X_test)\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f49da73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      1.00      0.62        82\n",
      "           1       0.00      0.00      0.00        53\n",
      "           2       0.00      0.00      0.00        22\n",
      "           3       0.00      0.00      0.00        21\n",
      "           4       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.45       184\n",
      "   macro avg       0.09      0.20      0.12       184\n",
      "weighted avg       0.20      0.45      0.27       184\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniqu/.conda/envs/fairness-analysis-rl/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/aniqu/.conda/envs/fairness-analysis-rl/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/aniqu/.conda/envs/fairness-analysis-rl/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, recall_score\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e25c6177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12330827067669173\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_test, preds, average='macro'))\n",
    "print(recall_score(y_test, preds, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245e47b4",
   "metadata": {},
   "source": [
    "### Gender original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c06c646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        accuracy  recall      f1  n_samples\n",
      "Male      0.3618  0.2000  0.1063      152.0\n",
      "Female    0.8438  0.3333  0.3051       32.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "X_test = pd.read_csv('../../data/X_test.csv')\n",
    "y_test = pd.read_csv('../../data/y_test.csv')\n",
    "\n",
    "preds_test = evaluate_model(model_orig, X_test_num)\n",
    "\n",
    "\n",
    "X_test[\"pred\"] = preds_test      \n",
    "X_test[\"true\"] = y_test   \n",
    "\n",
    "# Protected attribute\n",
    "protected_attr = \"sex_Male\"  \n",
    "groups = X_test[protected_attr].unique()\n",
    "\n",
    "metrics = {}\n",
    "for g in groups:\n",
    "    group_df = X_test[X_test[protected_attr] == g]\n",
    "    n_samples = len(group_df)\n",
    "    acc = accuracy_score(group_df[\"true\"], group_df[\"pred\"])\n",
    "    rec = recall_score(group_df[\"true\"], group_df[\"pred\"], average=\"macro\")  \n",
    "    f1 = f1_score(group_df[\"true\"], group_df[\"pred\"], average=\"macro\")  \n",
    "    label = \"Male\" if g == 1 else \"Female\"\n",
    "    metrics[label] = {\n",
    "        \"accuracy\": round(acc, 4),\n",
    "        \"recall\": round(rec, 4),\n",
    "        \"f1\": round(f1, 4),\n",
    "        \"n_samples\": int(n_samples)\n",
    "    }\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "print(metrics_df)\n",
    "\n",
    "# save\n",
    "with open(\"../../results/dqn_gender_original.json\", \"w\") as f:\n",
    "    json.dump(metrics_df.to_dict(), f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3151501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = -4.985, p = 0.0000006197\n",
      "accuracy difference significant\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "\n",
    "male_mask = X_test[\"sex_Male\"] == 1\n",
    "female_mask = X_test[\"sex_Male\"] == 0\n",
    "\n",
    "\n",
    "y_true_male = y_test[male_mask]\n",
    "y_pred_male = preds_test[male_mask]\n",
    "y_true_female = y_test[female_mask]\n",
    "y_pred_female = preds_test[female_mask]\n",
    "\n",
    "assert((len(y_true_male) + len(y_true_female)) == (len(y_pred_male) + len(y_pred_female)))\n",
    "\n",
    "acc_male = (y_pred_male == y_true_male.to_numpy().ravel()).astype(int)\n",
    "acc_female = (y_pred_female == y_true_female.to_numpy().ravel()).astype(int)\n",
    "\n",
    "\n",
    "count = np.array([acc_male.sum(), acc_female.sum()])  # number of correct predictions\n",
    "nobs = np.array([len(acc_male), len(acc_female)])      # group sizes\n",
    "\n",
    "stat, pval = proportions_ztest(count, nobs)\n",
    "print(f\"z = {stat:.3f}, p = {pval:.10f}\")\n",
    "\n",
    "if pval < 0.05:\n",
    "    print('accuracy difference significant')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2c96fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.92        27\n",
      "           1       0.00      0.00      0.00         4\n",
      "           2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.84        32\n",
      "   macro avg       0.28      0.33      0.31        32\n",
      "weighted avg       0.71      0.84      0.77        32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniqu/.conda/envs/fairness-analysis-rl/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/aniqu/.conda/envs/fairness-analysis-rl/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/aniqu/.conda/envs/fairness-analysis-rl/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(X_test[X_test['sex_Male'] == 0]['true'], X_test[X_test['sex_Male'] == 0]['pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935c7a02",
   "metadata": {},
   "source": [
    "## Age groups original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cc2d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/age_analysis/Xy_test_age_analysis.csv')\n",
    "df.rename(columns={'num':'true'}, inplace=True)\n",
    "df['pred'] = preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e64dc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       accuracy  recall        f1  n_samples\n",
      "<40    0.666667    0.50  0.400000       18.0\n",
      "40-50  0.611111    0.25  0.189655       36.0\n",
      "50-60  0.437500    0.20  0.121739       80.0\n",
      "60+    0.260000    0.20  0.082540       50.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "results = {}\n",
    "\n",
    "for group in df['age_group'].unique():\n",
    "    mask = df['age_group'] == group\n",
    "    y_true = df.loc[mask, 'true']\n",
    "    y_pred = df.loc[mask, 'pred']\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    n_samples = len(y_true)\n",
    "\n",
    "    results[group] = {'accuracy': acc, 'recall': rec, 'f1': f1, 'n_samples': n_samples}\n",
    "\n",
    "age_results = pd.DataFrame(results).T\n",
    "order = [0, 3, 2, 1]\n",
    "age_results_ordered = age_results.iloc[order]\n",
    "print(age_results_ordered)\n",
    "\n",
    "with open(\"../../results/dqn_age_original.json\", \"w\") as f:\n",
    "    json.dump(age_results_ordered.to_dict(), f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62ae72bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<40 vs 40-50: z = 0.3985, p = 0.690242 → ❌ \n",
      "<40 vs 50-60: z = 1.7584, p = 0.078684 → ❌ \n",
      "<40 vs 60+: z = 3.0684, p = 0.002152 → ✅ \n",
      "40-50 vs 50-60: z = 1.7304, p = 0.083564 → ❌ \n",
      "40-50 vs 60+: z = 3.2697, p = 0.001077 → ✅ \n",
      "50-60 vs 60+: z = 2.0402, p = 0.041331 → ✅ \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "age_groups = [\"<40\", \"40-50\", \"50-60\", \"60+\"]\n",
    "results = []\n",
    "\n",
    "for g1, g2 in itertools.combinations(age_groups, 2):\n",
    "    mask_1 = df[\"age_group\"] == g1\n",
    "    mask_2 = df[\"age_group\"] == g2\n",
    "\n",
    "    y_true_1 = df.loc[mask_1, \"true\"]\n",
    "    y_pred_1 = df.loc[mask_1, \"pred\"]\n",
    "    y_true_2 = df.loc[mask_2, \"true\"]\n",
    "    y_pred_2 = df.loc[mask_2, \"pred\"]\n",
    "\n",
    "    acc_1 = (y_pred_1.to_numpy() == y_true_1.to_numpy()).astype(int)\n",
    "    acc_2 = (y_pred_2.to_numpy() == y_true_2.to_numpy()).astype(int)\n",
    "\n",
    "    count = np.array([acc_1.sum(), acc_2.sum()])\n",
    "    nobs = np.array([len(acc_1), len(acc_2)])\n",
    "    stat, pval = proportions_ztest(count, nobs)\n",
    "\n",
    "    acc_rate_1 = acc_1.mean()\n",
    "    acc_rate_2 = acc_2.mean()\n",
    "    \n",
    "    result = {\n",
    "        \"comparison\": f\"{g1} vs {g2}\",\n",
    "        \"accuracy_1\": f\"{acc_rate_1:.6f}\",\n",
    "        \"accuracy_2\": f\"{acc_rate_2:.6f}\",\n",
    "        \"z_stat\": f\"{stat:.6f}\",\n",
    "        \"p_value\": f\"{pval:.8f}\",\n",
    "        \"significant\": bool(pval < 0.05)\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    # print to console\n",
    "    symbol = \"✅\" if pval < 0.05 else \"❌\"\n",
    "    print(f\"{g1} vs {g2}: z = {stat:.4f}, p = {pval:.6f} → {symbol} \")\n",
    "\n",
    "with open(\"../../results/dqn_age_original_significance.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairness-analysis-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
